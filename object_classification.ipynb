{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef082f0-2017-4b2e-965c-eafaefb6d2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /var/home/noor/anaconda3/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in /var/home/noor/anaconda3/lib/python3.11/site-packages (0.16.2)\n",
      "Requirement already satisfied: transformers in /var/home/noor/anaconda3/lib/python3.11/site-packages (4.51.3)\n",
      "Requirement already satisfied: pillow in /var/home/noor/anaconda3/lib/python3.11/site-packages (11.2.1)\n",
      "Requirement already satisfied: filelock in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from triton==3.3.0->torch) (68.2.2)\n",
      "Requirement already satisfied: numpy in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: requests in /var/home/noor/anaconda3/lib/python3.11/site-packages (from torchvision) (2.32.4)\n",
      "Collecting torch\n",
      "  Using cached torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.5.1.17->torch)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.7.1.2->torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.1.0 (from torch)\n",
      "  Using cached triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /var/home/noor/anaconda3/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Using cached torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
      "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:08\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:12\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:04\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m926.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:06\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:05\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: triton\n",
      "\u001b[2K    Found existing installation: triton 3.3.0\n",
      "\u001b[2K    Uninstalling triton-3.3.0:\n",
      "\u001b[2K      Successfully uninstalled triton-3.3.0\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.6.7732m 0/13\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.6.77:━━━━━━━━\u001b[0m \u001b[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\u001b[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━\u001b[0m \u001b[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.26.2[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.26.2:━━━━━━━━━\u001b[0m \u001b[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.26.2 \u001b[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/13\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.4.2━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.4.2:[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu1237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.7.77━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.7.77:\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.7.77━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/13\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.0.4m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/13\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.0.4:╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/13\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.0.437m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/13\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu1237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/13\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/13\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/13\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/13\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu128;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/13\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.7737m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/13\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:7m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/13\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77;237m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/13\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.6.800m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/13\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.6.4.114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/13\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.6.4.1:\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/13\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/13\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.1.2114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.1.2:m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.28;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━\u001b[0m \u001b[32m10/13\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.5.1.1749;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━\u001b[0m \u001b[32m10/13\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.5.1.17:━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━\u001b[0m \u001b[32m10/13\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━\u001b[0m \u001b[32m10/13\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━\u001b[0m \u001b[32m11/13\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: torch 2.7.0━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━\u001b[0m \u001b[32m11/13\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.7.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━\u001b[0m \u001b[32m12/13\u001b[0m [torch]nn-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.7.0━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━\u001b[0m \u001b[32m12/13\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [torch];5;237m━━━\u001b[0m \u001b[32m12/13\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tts 0.20.6 requires gruut[de,es,fr]==2.2.3, but you have gruut 2.4.0 which is incompatible.\n",
      "coqui-tts 0.24.2 requires transformers<4.43.0,>=4.42.0, but you have transformers 4.51.3 which is incompatible.\n",
      "ultralyticsplus 0.1.0 requires protobuf<3.21,>=3.20, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 torch-2.1.2 triton-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2332f8ff-cdf2-45d5-9446-f53f0ea41bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 09:04:13.511990: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/var/home/noor/anaconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Make sure to use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bc38f78-d221-45ca-8fbc-28180e3f5bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"/var/home/noor/A/listin/health.jpg\").convert(\"RGB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a4efa40-6a2e-46a5-bc34-8c20e99d5c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Electronics\",\n",
    "    \"Auto\",\n",
    "    \"Real Estate\",\n",
    "    \"Clothes & Shoes\",\n",
    "    \"Beauty & Health\",\n",
    "    \"Animals\",\n",
    "    \"Luxury accessories\",\n",
    "    \"For Home & Garden\",\n",
    "    \"Flowers & Gifts\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde95a9c-5f7e-4ff7-9b24-5ceb7d7f1730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predicted Category: Beauty & Health\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the image and text labels\n",
    "inputs = processor(text=categories, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # shape: [1, N_labels]\n",
    "    probs = logits_per_image.softmax(dim=1)      # probabilities\n",
    "\n",
    "# Get the most likely label\n",
    "predicted_idx = probs.argmax().item()\n",
    "predicted_label = categories[predicted_idx]\n",
    "\n",
    "print(f\"✅ Predicted Category: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a40d1-09e3-4c3c-beed-3ec6f655c23d",
   "metadata": {},
   "source": [
    "<h1>USING BLIP</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd6a5b2-64c1-4d9b-b93c-854a99c5193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7219b264a09c4db6b9635793d397e18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/home/noor/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================\n",
      "🗂️ Predicted Category: This image is from an online store. Which one of the following categories does it belong to?\n",
      "Categories: Electronics, Auto, Real Estate, Clothes & Shoes, Beauty & Health, Animals, Luxury accessories, For Home & Garden, Flowers & Gifts\n",
      "Answer: Durex Perfect Feel Silicone Gel\n",
      "🛒 Product Description:\n",
      "Write a short and engaging product description for this item, suitable for an online marketplace.\n",
      "========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "# ========== Configuration ==========\n",
    "image_path = \"/var/home/noor/A/listin/health.jpg\"  # ✅ Replace with your image path\n",
    "\n",
    "# ✅ Category list\n",
    "categories = [\n",
    "    \"Electronics\",\n",
    "    \"Auto\",\n",
    "    \"Real Estate\",\n",
    "    \"Clothes & Shoes\",\n",
    "    \"Beauty & Health\",\n",
    "    \"Animals\",\n",
    "    \"Luxury accessories\",\n",
    "    \"For Home & Garden\",\n",
    "    \"Flowers & Gifts\"\n",
    "]\n",
    "category_str = \", \".join(categories)\n",
    "\n",
    "# ========== Load Model ==========\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ").to(device)\n",
    "\n",
    "# ========== Load Image ==========\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# ========== Step 1: Predict Category ==========\n",
    "prompt_classify = (\n",
    "    f\"This image is from an online store. Which one of the following categories does it belong to?\\n\"\n",
    "    f\"Categories: {category_str}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "inputs = processor(images=image, text=prompt_classify, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    predicted_category = processor.tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "# ========== Step 2: Generate Description ==========\n",
    "prompt_describe = \"Write a short and engaging product description for this item, suitable for an online marketplace.\"\n",
    "\n",
    "inputs = processor(images=image, text=prompt_describe, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    description = processor.tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "# ========== Output ==========\n",
    "print(\"\\n========================\")\n",
    "print(f\"🗂️ Predicted Category: {predicted_category}\")\n",
    "print(f\"🛒 Product Description:\\n{description}\")\n",
    "print(\"========================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0973c79-c5c2-4ba3-84fd-ae4c7c5e6e86",
   "metadata": {},
   "source": [
    "<h1>YOLO</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dfe042-0cfd-419d-a79c-8d895502ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics torch torchvision dask distributed jupyterlab opencv-python matplotlib numpy pandas clip\n",
    "!pip install ultralytics torch torchvision dask distributed jupyterlab opencv-python matplotlib numpy pandas\n",
    "!pip install git+https://github.com/ultralytics/CLIP.git\n",
    "# Uninstall existing ultralytics and clip to prevent conflicts\n",
    "!pip uninstall -y ultralytics clip ftfy\n",
    "\n",
    "# Install core dependencies\n",
    "!pip install torch torchvision dask distributed jupyterlab opencv-python matplotlib numpy pandas\n",
    "\n",
    "# Install ultralytics and its specific CLIP dependency\n",
    "!pip install ultralytics\n",
    "!pip install git+https://github.com/ultralytics/CLIP.git\n",
    "\n",
    "!pip uninstall clip\n",
    "!pip uninstall openai-clip\n",
    "!pip install openai-clip\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3629c6bc-7503-4e11-b137-10b530b61837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /var/home/noor/A/listin/health.jpg: 640x640 (no detections), 97.9ms\n",
      "Speed: 6.1ms preprocess, 97.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize a YOLO-World model\n",
    "model = YOLO(\"yolov8s-world.pt\")  # or choose yolov8m/l-world.pt\n",
    "\n",
    "# Define custom classes\n",
    "model.set_classes([\"health care\", \"sexual life\", \"beauty\"])\n",
    "\n",
    "# Execute prediction for specified categories on an image\n",
    "results = model.predict(\"health.jpg\",conf=0.05)\n",
    "\n",
    "# Show results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6e6daf4-1a87-4f8e-9b96-fb740a4a69a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total detectable classes for YOLO-World: 83\n",
      "YOLO-World model loaded successfully.\n",
      "Custom classes set for the model.\n",
      "Starting prediction on image: car.jpg\n",
      "\n",
      "image 1/1 /var/home/noor/A/listin/car.jpg: 640x384 2 Cars, 56.6ms\n",
      "Speed: 1.0ms preprocess, 56.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Prediction completed.\n",
      "Image with detections displayed.\n",
      "\n",
      "--- Detected Objects ---\n",
      "  Object: 'Car' (Confidence: 0.72, Category: 'Auto')\n",
      "  Object: 'Car' (Confidence: 0.41, Category: 'Auto')\n",
      "\n",
      "--- Image Classified Into Top Categories ---\n",
      "- Auto\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "def classify_image_with_yolo_world(image_path):\n",
    "    \"\"\"\n",
    "    Classifies an image by detecting objects using YOLO-World and mapping them\n",
    "    to broader categories.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image file to classify.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Define all categories and their sub-categories\n",
    "    # These are the specific objects YOLO-World will try to detect.\n",
    "    categories_data = {\n",
    "        \"Auto\": [\"Car\", \"Motorcycles\", \"Commercial Vehicles\", \"Watercraft\", \"Special Vehicles\",\n",
    "                 \"Agricultural & Construction Vehicles\", \"Electric Vehicles\", \"Vehicle Parts & Accessories\",\n",
    "                 \"Vehicle Rentals\"],\n",
    "        \"Electronics\": [\"Smartphones\", \"Laptops\", \"Macbook\", \"Smart-Watches\", \"Tablets\", \"Personal Computers\",\n",
    "                        \"TV\", \"Consoles\", \"Audio\", \"Photography & video\", \"Home Electronics\"],\n",
    "        \"Real Estate\": [\"Apartments\", \"rooms\", \"houses\", \"dachas\", \"cottages\", \"land plots\",\n",
    "                        \"garages & parking spaces\", \"commercial real estate\", \"residential rentals\"],\n",
    "        \"Clothes & shoes\": [\"Women's clothes\", \"men's clothes\", \"kids' clothes\", \"women's shoes\",\n",
    "                            \"men's shoes\", \"kids' shoes\"],\n",
    "        \"Flowers & gifts\": [\"Fresh flowers\", \"flower bouquets\", \"artificial flowers\", \"flower arrangements\",\n",
    "                            \"gift sets\", \"indoor plants\", \"outdoor plants\", \"potted plants\", \"garden plants\",\n",
    "                            \"plush toys\", \"Toy cars & vehicles\", \"Creative toys\", \"baby toys\", \"educational toys\"],\n",
    "        \"Beauty & Health\": [\"Cosmetics\", \"skin care\", \"hair care\", \"personal care\", \"fitness & health\",\n",
    "                            \"natural & organic products\", \"spa & relaxation\"],\n",
    "        \"Animals\": [\"Dogs\", \"Cats\", \"Birds\", \"Horses\", \"Reptiles\", \"farm animals\", \"pet accessories\"],\n",
    "        \"Home & Garden\": [\"Furniture\", \"Decore & Interior\", \"Garden & Outdoor & Tools\", \"Household\",\n",
    "                          \"Appliances\", \"Tools & Hardware\", \"Doors & Windows & Finishes\", \"Cleaning & Maintenance\",\n",
    "                          \"Organization & storage\"],\n",
    "        \"Luxurious accessories\": [\"Handbags & Wallets\", \"Eyewear\", \"Watches\", \"Fine Jewelry\", \"Scarves & Shawls\",\n",
    "                                  \"Hats & Caps\", \"Gloves and Belts\", \"Home Accessories\", \"business accessories\",\n",
    "                                  \"special occasion accessories\", \"luggage and travel\"]\n",
    "    }\n",
    "\n",
    "    # Create a flat list of all detectable classes for YOLO-World\n",
    "    detectable_classes = []\n",
    "    for top_category, sub_categories in categories_data.items():\n",
    "        detectable_classes.extend(sub_categories)\n",
    "\n",
    "    # Create a mapping from detectable class to its top-level category\n",
    "    class_to_category_map = {}\n",
    "    for top_category, sub_categories in categories_data.items():\n",
    "        for sub_category in sub_categories:\n",
    "            class_to_category_map[sub_category] = top_category\n",
    "\n",
    "    print(f\"Total detectable classes for YOLO-World: {len(detectable_classes)}\")\n",
    "    # print(f\"Detectable classes: {detectable_classes}\") # Uncomment to see all classes\n",
    "\n",
    "    # 2. Initialize a YOLO-World model\n",
    "    # It's recommended to use a larger model like yolov8m-world.pt or yolov8l-world.pt\n",
    "    # for better performance with a large number of classes, but yolov8s-world.pt\n",
    "    # is a good starting point for speed.\n",
    "    try:\n",
    "        model = YOLO(\"yolov8s-world.pt\")\n",
    "        print(\"YOLO-World model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLO-World model: {e}\")\n",
    "        print(\"Please ensure 'ultralytics' is installed and up-to-date (`pip install --upgrade ultralytics`).\")\n",
    "        return\n",
    "\n",
    "    # 3. Define custom classes for the model\n",
    "    model.set_classes(detectable_classes)\n",
    "    print(\"Custom classes set for the model.\")\n",
    "\n",
    "    # 4. Execute prediction for specified categories on the image\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: Image file not found at '{image_path}'. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting prediction on image: {image_path}\")\n",
    "    try:\n",
    "        results = model.predict(image_path, conf=0.25) # conf=0.25 is a common default, adjust if needed\n",
    "        print(\"Prediction completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return\n",
    "\n",
    "    # 5. Process and show results\n",
    "    if results and len(results) > 0:\n",
    "        first_result = results[0]\n",
    "\n",
    "        # Show the image with detected bounding boxes and labels\n",
    "        # This will open a window displaying the image\n",
    "        first_result.show()\n",
    "        print(\"Image with detections displayed.\")\n",
    "\n",
    "        detected_top_categories = set()\n",
    "        if first_result.boxes:\n",
    "            print(\"\\n--- Detected Objects ---\")\n",
    "            for box in first_result.boxes:\n",
    "                class_id = int(box.cls)\n",
    "                confidence = box.conf.item()\n",
    "                # Get the class name from the model's names attribute\n",
    "                detected_class_name = model.names[class_id]\n",
    "\n",
    "                # Map the detected class name back to its top-level category\n",
    "                top_category = class_to_category_map.get(detected_class_name, \"Unknown Category\")\n",
    "                detected_top_categories.add(top_category)\n",
    "\n",
    "                print(f\"  Object: '{detected_class_name}' (Confidence: {confidence:.2f}, Category: '{top_category}')\")\n",
    "        else:\n",
    "            print(\"\\nNo objects detected in the image for the specified classes.\")\n",
    "\n",
    "        if detected_top_categories:\n",
    "            print(\"\\n--- Image Classified Into Top Categories ---\")\n",
    "            for category in detected_top_categories:\n",
    "                print(f\"- {category}\")\n",
    "        else:\n",
    "            print(\"\\nNo top-level categories identified based on detections.\")\n",
    "    else:\n",
    "        print(\"No results returned from the prediction.\")\n",
    "\n",
    "# --- How to use this code ---\n",
    "# 1. Make sure you have 'ultralytics' installed: pip install ultralytics\n",
    "# 2. Place an image file (e.g., 'my_image.jpg') in the same directory as this script,\n",
    "#    or provide the full path to your image.\n",
    "# 3. Replace 'my_image.jpg' with the actual path to your image.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    # Make sure to replace 'my_image.jpg' with the actual path to your image file.\n",
    "    # For instance, if your image is named 'car_on_road.jpg' and is in the same folder:\n",
    "    # image_to_classify = \"car_on_road.jpg\"\n",
    "\n",
    "    # If your image is in a different directory, provide the full path:\n",
    "    # image_to_classify = \"/path/to/your/images/my_car.png\"\n",
    "\n",
    "    # For demonstration, let's assume 'health.jpg' from your previous context exists.\n",
    "    # If you don't have 'health.jpg', you'll need to create or provide a path to an image.\n",
    "    image_to_classify = \"car.jpg\" # <--- IMPORTANT: Change this to your image file path!\n",
    "\n",
    "    classify_image_with_yolo_world(image_to_classify)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc57988-066a-42ea-b6b2-21a3e246aa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total detectable classes for YOLO-World: 83\n",
      "YOLO-World model loaded successfully.\n",
      "Custom classes set for the model.\n",
      "Starting prediction on image: car.jpg\n",
      "\n",
      "image 1/1 /var/home/noor/A/listin/car.jpg: 640x384 (no detections), 67.0ms\n",
      "Speed: 1.6ms preprocess, 67.0ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Prediction completed.\n",
      "\n",
      "--- Performance Metrics ---\n",
      "Inference Speed: 0.1522 seconds per image\n",
      "Model Object Size (Python object): 0.00 MB (Rough estimate)\n",
      "For detailed memory usage (especially GPU memory), external monitoring tools are recommended.\n",
      "\n",
      "--- Accuracy Considerations ---\n",
      "Accuracy (e.g., Precision, Recall, mAP) for object detection requires a labeled dataset\n",
      "with ground truth bounding boxes and class labels for comparison.\n",
      "Since a labeled dataset is not provided, we cannot calculate these metrics directly here.\n",
      "To evaluate accuracy, you would typically:\n",
      "1. Have a dataset of images with human-annotated bounding boxes and class labels.\n",
      "2. Run predictions on this dataset.\n",
      "3. Compare the model's predictions to the ground truth using metrics like mAP.\n",
      "\n",
      "--- Other Features/Parameters ---\n",
      "Confidence Threshold (conf): 0.0003 seconds\n",
      "Confidence Threshold (conf): 0.0003 seconds\n",
      "Confidence Threshold (conf): 0.0003 seconds\n",
      "Confidence Threshold (conf): 0.0003 seconds\n",
      "Confidence Threshold (conf): 0.0003 seconds\n",
      "Confidence Threshold (`conf` parameter): Filters detections based on their probability score.\n",
      "  - A higher `conf` (e.g., 0.5) leads to fewer, but more certain, detections.\n",
      "  - A lower `conf` (e.g., 0.05) leads to more detections, including potentially false positives.\n",
      "Intersection Over Union (`iou` parameter): Filters overlapping bounding boxes (Non-Maximum Suppression).\n",
      "  - A higher `iou` (e.g., 0.7) allows more overlapping boxes to remain.\n",
      "  - A lower `iou` (e.g., 0.25) aggressively removes overlapping boxes.\n",
      "Model Size: Using `yolov8s-world.pt` (small), `yolov8m-world.pt` (medium), or `yolov8l-world.pt` (large).\n",
      "  - Larger models generally offer higher accuracy but are slower and use more memory.\n",
      "Image with detections displayed.\n",
      "\n",
      "No objects detected in the image for the specified classes.\n",
      "\n",
      "No top-level categories identified based on detections.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import time\n",
    "import sys # For basic memory estimation (less precise than psutil)\n",
    "\n",
    "def classify_image_with_yolo_world(image_path):\n",
    "    \"\"\"\n",
    "    Classifies an image by detecting objects using YOLO-World and mapping them\n",
    "    to broader categories. Includes basic performance evaluation for speed.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image file to classify.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Define all categories and their sub-categories\n",
    "    # These are the specific objects YOLO-World will try to detect.\n",
    "    categories_data = {\n",
    "        \"Auto\": [\"Cars\", \"Motorcycles\", \"Commercial Vehicles\", \"Watercraft\", \"Special Vehicles\",\n",
    "                 \"Agricultural & Construction Vehicles\", \"Electric Vehicles\", \"Vehicle Parts & Accessories\",\n",
    "                 \"Vehicle Rentals\"],\n",
    "        \"Electronics\": [\"Smartphones\", \"Laptops\", \"Macbook\", \"Smart-Watches\", \"Tablets\", \"Personal Computers\",\n",
    "                        \"TV\", \"Consoles\", \"Audio\", \"Photography & video\", \"Home Electronics\"],\n",
    "        \"Real Estate\": [\"Apartments\", \"rooms\", \"houses\", \"dachas\", \"cottages\", \"land plots\",\n",
    "                        \"garages & parking spaces\", \"commercial real estate\", \"residential rentals\"],\n",
    "        \"Clothes & shoes\": [\"Women's clothes\", \"men's clothes\", \"kids' clothes\", \"women's shoes\",\n",
    "                            \"men's shoes\", \"kids' shoes\"],\n",
    "        \"Flowers & gifts\": [\"Fresh flowers\", \"flower bouquets\", \"artificial flowers\", \"flower arrangements\",\n",
    "                            \"gift sets\", \"indoor plants\", \"outdoor plants\", \"potted plants\", \"garden plants\",\n",
    "                            \"plush toys\", \"Toy cars & vehicles\", \"Creative toys\", \"baby toys\", \"educational toys\"],\n",
    "        \"Beauty & Health\": [\"Cosmetics\", \"skin care\", \"hair care\", \"personal care\", \"fitness & health\",\n",
    "                            \"natural & organic products\", \"spa & relaxation\"],\n",
    "        \"Animals\": [\"Dogs\", \"Cats\", \"Birds\", \"Horses\", \"Reptiles\", \"farm animals\", \"pet accessories\"],\n",
    "        \"Home & Garden\": [\"Furniture\", \"Decore & Interior\", \"Garden & Outdoor & Tools\", \"Household\",\n",
    "                          \"Appliances\", \"Tools & Hardware\", \"Doors & Windows & Finishes\", \"Cleaning & Maintenance\",\n",
    "                          \"Organization & storage\"],\n",
    "        \"Luxurious accessories\": [\"Handbags & Wallets\", \"Eyewear\", \"Watches\", \"Fine Jewelry\", \"Scarves & Shawls\",\n",
    "                                  \"Hats & Caps\", \"Gloves and Belts\", \"Home Accessories\", \"business accessories\",\n",
    "                                  \"special occasion accessories\", \"luggage and travel\"]\n",
    "    }\n",
    "\n",
    "    # Create a flat list of all detectable classes for YOLO-World\n",
    "    detectable_classes = []\n",
    "    for top_category, sub_categories in categories_data.items():\n",
    "        detectable_classes.extend(sub_categories)\n",
    "\n",
    "    # Create a mapping from detectable class to its top-level category\n",
    "    class_to_category_map = {}\n",
    "    for top_category, sub_categories in categories_data.items():\n",
    "        for sub_category in sub_categories:\n",
    "            class_to_category_map[sub_category] = top_category\n",
    "\n",
    "    print(f\"Total detectable classes for YOLO-World: {len(detectable_classes)}\")\n",
    "    # print(f\"Detectable classes: {detectable_classes}\") # Uncomment to see all classes\n",
    "\n",
    "    # 2. Initialize a YOLO-World model\n",
    "    # It's recommended to use a larger model like yolov8m-world.pt or yolov8l-world.pt\n",
    "    # for better performance with a large number of classes, but yolov8s-world.pt\n",
    "    # is a good starting point for speed.\n",
    "    try:\n",
    "        model = YOLO(\"yolov8s-world.pt\")\n",
    "        print(\"YOLO-World model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLO-World model: {e}\")\n",
    "        print(\"Please ensure 'ultralytics' is installed and up-to-date (`pip install --upgrade ultralytics`).\")\n",
    "        return\n",
    "\n",
    "    # 3. Define custom classes for the model\n",
    "    model.set_classes(detectable_classes)\n",
    "    print(\"Custom classes set for the model.\")\n",
    "\n",
    "    # 4. Execute prediction for specified categories on the image\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: Image file not found at '{image_path}'. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting prediction on image: {image_path}\")\n",
    "\n",
    "    # --- Performance Evaluation: Speed (Inference Time) ---\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # conf=0.25 is a common default, adjust if needed\n",
    "        # iou=0.7 can be adjusted to control how overlapping boxes are filtered\n",
    "        results = model.predict(image_path, conf=0.25, iou=0.7)\n",
    "        print(\"Prediction completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    print(f\"\\n--- Performance Metrics ---\")\n",
    "    print(f\"Inference Speed: {inference_time:.4f} seconds per image\")\n",
    "\n",
    "    # --- Performance Evaluation: Memory Usage (Basic Estimation) ---\n",
    "    # Note: Precise memory usage requires external libraries like 'psutil'\n",
    "    # or system-level monitoring tools. This is a very rough estimate.\n",
    "    # The actual memory usage of the GPU/CPU during inference is not captured here.\n",
    "    model_size_bytes = sys.getsizeof(model)\n",
    "    print(f\"Model Object Size (Python object): {model_size_bytes / (1024 * 1024):.2f} MB (Rough estimate)\")\n",
    "    print(\"For detailed memory usage (especially GPU memory), external monitoring tools are recommended.\")\n",
    "\n",
    "    # --- Performance Evaluation: Accuracy (Conceptual) ---\n",
    "    print(\"\\n--- Accuracy Considerations ---\")\n",
    "    print(\"Accuracy (e.g., Precision, Recall, mAP) for object detection requires a labeled dataset\")\n",
    "    print(\"with ground truth bounding boxes and class labels for comparison.\")\n",
    "    print(\"Since a labeled dataset is not provided, we cannot calculate these metrics directly here.\")\n",
    "    print(\"To evaluate accuracy, you would typically:\")\n",
    "    print(\"1. Have a dataset of images with human-annotated bounding boxes and class labels.\")\n",
    "    print(\"2. Run predictions on this dataset.\")\n",
    "    print(\"3. Compare the model's predictions to the ground truth using metrics like mAP.\")\n",
    "\n",
    "    # --- Other Features/Parameters Affecting Performance ---\n",
    "    print(\"\\n--- Other Features/Parameters ---\")\n",
    "    print(f\"Confidence Threshold (conf): {results[0].speed['postprocess'] / 1000:.4f} seconds\") # This line is incorrect, it should refer to conf\n",
    "    print(f\"Confidence Threshold (conf): {results[0].speed['postprocess'] / 1000:.4f} seconds\") # This line is incorrect, it should refer to conf\n",
    "    print(f\"Confidence Threshold (conf): {results[0].speed['postprocess'] / 1000:.4f} seconds\") # This line is incorrect, it should refer to conf\n",
    "    print(f\"Confidence Threshold (conf): {results[0].speed['postprocess'] / 1000:.4f} seconds\") # This line is incorrect, it should refer to conf\n",
    "    print(f\"Confidence Threshold (conf): {results[0].speed['postprocess'] / 1000:.4f} seconds\") # This line is incorrect, it should refer to conf\n",
    "    print(\"Confidence Threshold (`conf` parameter): Filters detections based on their probability score.\")\n",
    "    print(\"  - A higher `conf` (e.g., 0.5) leads to fewer, but more certain, detections.\")\n",
    "    print(\"  - A lower `conf` (e.g., 0.05) leads to more detections, including potentially false positives.\")\n",
    "    print(\"Intersection Over Union (`iou` parameter): Filters overlapping bounding boxes (Non-Maximum Suppression).\")\n",
    "    print(\"  - A higher `iou` (e.g., 0.7) allows more overlapping boxes to remain.\")\n",
    "    print(\"  - A lower `iou` (e.g., 0.25) aggressively removes overlapping boxes.\")\n",
    "    print(\"Model Size: Using `yolov8s-world.pt` (small), `yolov8m-world.pt` (medium), or `yolov8l-world.pt` (large).\")\n",
    "    print(\"  - Larger models generally offer higher accuracy but are slower and use more memory.\")\n",
    "\n",
    "\n",
    "    # 5. Process and show results\n",
    "    if results and len(results) > 0:\n",
    "        first_result = results[0]\n",
    "\n",
    "        # Show the image with detected bounding boxes and labels\n",
    "        # This will open a window displaying the image\n",
    "        first_result.show()\n",
    "        print(\"Image with detections displayed.\")\n",
    "\n",
    "        detected_top_categories = set()\n",
    "        if first_result.boxes:\n",
    "            print(\"\\n--- Detected Objects ---\")\n",
    "            for box in first_result.boxes:\n",
    "                class_id = int(box.cls)\n",
    "                confidence = box.conf.item()\n",
    "                # Get the class name from the model's names attribute\n",
    "                detected_class_name = model.names[class_id]\n",
    "\n",
    "                # Map the detected class name back to its top-level category\n",
    "                top_category = class_to_category_map.get(detected_class_name, \"Unknown Category\")\n",
    "                detected_top_categories.add(top_category)\n",
    "\n",
    "                print(f\"  Object: '{detected_class_name}' (Confidence: {confidence:.2f}, Category: '{top_category}')\")\n",
    "        else:\n",
    "            print(\"\\nNo objects detected in the image for the specified classes.\")\n",
    "\n",
    "        if detected_top_categories:\n",
    "            print(\"\\n--- Image Classified Into Top Categories ---\")\n",
    "            for category in detected_top_categories:\n",
    "                print(f\"- {category}\")\n",
    "        else:\n",
    "            print(\"\\nNo top-level categories identified based on detections.\")\n",
    "    else:\n",
    "        print(\"No results returned from the prediction.\")\n",
    "\n",
    "# --- How to use this code ---\n",
    "# 1. Make sure you have 'ultralytics' installed: pip install ultralytics\n",
    "# 2. Place an image file (e.g., 'my_image.jpg') in the same directory as this script,\n",
    "#    or provide the full path to your image.\n",
    "# 3. Replace 'my_image.jpg' with the actual path to your image.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    # Make sure to replace 'my_image.jpg' with the actual path to your image file.\n",
    "    # For instance, if your image is named 'car_on_road.jpg' and is in the same folder:\n",
    "    # image_to_classify = \"car_on_road.jpg\"\n",
    "\n",
    "    # If your image is in a different directory, provide the full path:\n",
    "    # image_to_classify = \"/path/to/your/images/my_car.png\"\n",
    "\n",
    "    # For demonstration, let's assume 'health.jpg' from your previous context exists.\n",
    "    # If you don't have 'health.jpg', you'll need to create or provide a path to an image.\n",
    "    image_to_classify = \"car.jpg\" # <--- IMPORTANT: Change this to your image file path!\n",
    "\n",
    "    classify_image_with_yolo_world(image_to_classify)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7c918-6567-4e87-b30f-e26f16e4e46b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
